# -*- coding: utf-8 -*-
"""Custom Individual - GA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K4FwUFO-rLofQqLFikH_snvyuU-EjEQb
"""
import random
import time
import itertools
import pandas as pd
import numpy as np
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL
import tensorflow as tf
import logging
import seaborn as sns
import lightgbm as lgb
from sklearn.svm import SVR
from tensorflow.keras.layers.experimental import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from deap import base
from deap import creator
from deap import tools
from itertools import chain
from lightgbm import LGBMRegressor

if not tf.test.gpu_device_name():
    print("asd")
else:
    print(tf.test.gpu_device_name())
    
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

tf.debugging.set_log_device_placement(True)

# Create some tensors
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print(c)
#os.chdir('C:\\Users\\speed\\OneDrive\\Escritorio\\TESIS\\Seminario 1\\Testing')
os.chdir('C:\\Users\\speed\\OneDrive\\Escritorio\\TESIS\\Seminario 2\\Implementacion')
os.getcwd()

np.set_printoptions(precision=3, suppress=True)

creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

<<<<<<< HEAD
#Crear agente ANN
=======
#Create individual
>>>>>>> 28a3b047bb08db011cc2da5af2cb2b0f715757be
class Agente(object):
    def __init__(self, num_input, max_num_hidden):
        self.num_input = num_input
        self.max_num_hidden = max_num_hidden

    def gen_hidden(self):
        return random.randint(1, self.max_num_hidden)
    
    def gen_activation(self): 
        return random.randint(0, 2)

    def gen_weights(self, num_hidden, type_):
        if type_ == "input":
            num_weights = self.num_input * num_hidden
        elif type_ == "output":
            num_weights = num_hidden
        result = [random.uniform(-1, 1) for i in range(num_weights)]
        #result = [random.randint(0,1) for i in range(num_weights)]
        return result

    def get_random_agent(self):
        num_hidden = self.gen_hidden()
        activation1 = self.gen_activation()
        activation2 = self.gen_activation()
        hidden_weights = self.gen_weights(num_hidden, type_="input")
        output_weights = self.gen_weights(num_hidden, type_="output")
        #print([[num_hidden], hidden_weights, output_weights])
        return [num_hidden] + [activation1] + [activation2] + hidden_weights + output_weights
#num_input, max_num_hidden
def newIndividual(num_input, max_num_hidden): 
    agent = Agente(num_input, max_num_hidden).get_random_agent()
    #print(agent)
    agent = creator.Individual(agent)
    return agent
print(newIndividual(16,10))
#with tf.device('/GPU:0'):
<<<<<<< HEAD
#Crear agente SVM
=======
>>>>>>> 28a3b047bb08db011cc2da5af2cb2b0f715757be
def agent_SVM(): 
    ind = []
    gamma = random.uniform(0.0001, 0.001)
    epsilon = random.uniform(0.0001, 0.001)
    C = random.uniform(0, 1)
    ind.append(gamma)
    ind.append(epsilon)
    ind.append(C)
    ind = creator.Individual(ind)
    return ind
print(agent_SVM())

<<<<<<< HEAD
#Crear agente GBT
=======
>>>>>>> 28a3b047bb08db011cc2da5af2cb2b0f715757be
def agent_GBT():
    ind = []
    learningRate = round(random.uniform(0.01, 1), 2)
    nEstimators = random.randrange(10, 1500, step = 25)
    maxDepth = int(random.randrange(1, 10, step= 1))
    minChildWeight = round(random.uniform(0.01, 10.0), 2)
    subSample = round(random.uniform(0.01, 1.0), 2)
    colSampleByTree = round(random.uniform(0.01, 1.0), 2)
    gammaValue = round(random.uniform(0.01, 10.0), 2)
    
    ind.append(learningRate)
    ind.append(nEstimators)
    ind.append(maxDepth)
    ind.append(minChildWeight)
    ind.append(subSample)
    ind.append(colSampleByTree)
    ind.append(gammaValue)
    ind = creator.Individual(ind)
    return ind

print(agent_GBT())

toolbox = base.Toolbox()
#Asignar class type deap.creator.Individual a nuestro individuo
<<<<<<< HEAD

## Registrar con ANN 
# Commentario: num_input, max_num_hidden
#toolbox.register("individual", newIndividual, 16, 10)

## Registrar con SVM
toolbox.register("individual", agent_SVM)

## Registrar con GBT
=======
#num_input, max_num_hidden
#toolbox.register("individual", newIndividual, 16, 10)
toolbox.register("individual", agent_SVM)
>>>>>>> 28a3b047bb08db011cc2da5af2cb2b0f715757be
#toolbox.register("individual", agent_GBT)
print(type(toolbox.individual()))
toolbox.individual()

#Crear population de tipo list con invdividuals de tipo deap.creator.Individual
toolbox.register("population", tools.initRepeat, list, toolbox.individual)
population = toolbox.population(n=20)
population

def neuralNetwork():
  dataset = pd.read_csv("DatasetFinal.csv")

  data = dataset.copy()

  X_full = data.drop(columns="Quota")
  y_full = data["Quota"]

  X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)
  X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25)
  
  #metrics = [tf.keras.metrics.MSE]
  #optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)

  return X_train, X_valid, X_test, y_train, y_valid, y_test

def mean_absolute_percentage_error(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    print(len(y_true))
    print(len(y_pred))
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

X_train, X_valid, X_test, y_train, y_valid, y_test = neuralNetwork()
print(y_test)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.model_selection import learning_curve
from sklearn.model_selection import validation_curve
from sklearn.model_selection import ShuffleSplit

def plot_learning_curve(estimator, title, X, y):
    #list(range(1,201))
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1, 10), scoring="neg_mean_squared_error")
    train_scores_mean = np.mean(train_scores, axis=1)
    #train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    #test_scores_std = np.std(test_scores, axis=1)
    
    plt.plot(train_sizes, -train_scores_mean, color="#C0C0C0",
             label="Training Loss")
    plt.plot(train_sizes, -test_scores_mean, color="k",
             label="Test loss")

    plt.xlabel("Training examples")
    plt.ylabel("Loss (MSE)")
    
    plt.legend(loc="best")
    plt.title(title)
    
    print("Train Scores")
    print(train_scores)
    
    print("Train Scores Mean")
    print(train_scores_mean)
    
    print("Test Scores")
    print(test_scores)
    
    print("Test Scores Mean")
    print(test_scores_mean)
    return plt

y_pred_svm = []
y_pred_ann = []
y_pred_gbt = []

"""# Support Vector Machine"""
def SVM(individual):
  gamma = individual[0]
  epsilon = individual[1]
  C = individual[2]
  print(gamma)
  print(epsilon)
  print(C)
  if (gamma <= 0 or epsilon <= 0 or C <= 0):
      gamma = 0.001
      epsilon = 0.001 
      C = 1 
  svm_reg = SVR(kernel='rbf', gamma=gamma, epsilon=epsilon, C=C, verbose=True) 
  svm_reg.fit(X_train, y_train)
  y_pred = svm_reg.predict(X_test)
  #train_scores, test_scores = validation_curve(svm_reg, X_train, y_train)
  #print("asd")
  #print(train_scores)
  #print(test_scores)
  
  X, y = X_train, y_train

  title = "Learning Curves (SVM, RBF kernel, C ={})".format(C)
  # SVC is more expensive so we do a lower number of CV iterations:
  #cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)

  plot_learning_curve(svm_reg, title, X, y)
  
  print("PREDICCION")
  print(len(y_pred))
  print(y_test)
  global y_pred_svm
  y_pred_svm = y_pred 
  
  print("VALOR")
  print(svm_reg.score)
  
  print("MSE: ", (mean_squared_error(y_test, y_pred)))
  print("RMSE: ", np.sqrt(mean_squared_error(y_test, y_pred)))
  print("MAE: ", mean_absolute_error(y_test, y_pred))
  print("MAPE: ", mean_absolute_percentage_error(y_test, y_pred))

  return mean_absolute_percentage_error(y_test, y_pred),
SVM([0.0, 0.0005662174972225848, 0.9483642075474367])

plt.show()

#print("MSE: ", 2638.149823742)
#print("RMSE: ", np.sqrt(2638.149823742))
#print("MAE: ", mean_absolute_error(y_test, y_pred))
#print("MAPE: ", mean_absolute_percentage_error(y_test, y_pred))

def plotGBTLoss(lgbm_reg, n_estimators):
    train_list = lgbm_reg.evals_result_['training']['l2']
    train_val_list = lgbm_reg.evals_result_['valid_1']['l2']
    
    #lgb.plot_metric(lgbm_reg)
    plt.plot(range(n_estimators), train_list, color="#C0C0C0", label="Train loss")
    plt.plot(range(n_estimators), train_val_list, color="k", label="Validation loss")

    plt.xlabel("Iterations")
    plt.ylabel("Loss (MSE)")
    
    plt.legend(loc="best")
    plt.title("Loss Curve GBT (Estimators: {})".format(n_estimators))

"""# Gradient Boosted Trees"""
def GBT(individual):
    learning_rate = individual[0]
    n_estimators = individual[1]
    max_depth = individual[2]
    min_child_weight = individual[3]
    subsample = individual[4]
    colsample_bytree = individual[5]
    gamma = individual[6]
    
    if (learning_rate <= 0 or subsample <= 0 or colsample_bytree <= 0 or n_estimators <= 0 or gamma <= 0 or min_child_weight <= 0 or max_depth <= 0):
        learning_rate = 0.01
        subsample = 0.01
        colsample_bytree = 0.01
        n_estimators = 10
        gamma = 0.01
        min_child_weight = 0.01
        max_depth = 1
        
    print(learning_rate)
    print(subsample)
    print(colsample_bytree) 
    
    lgbm_reg = LGBMRegressor(
        random_state=42,
        objective='mse',
        learning_rate=learning_rate,
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_child_weight=min_child_weight,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        gamma=gamma
    )
    
    #Entrenamiento de GBT
    lgbm_reg.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_valid, y_valid)],
        eval_metric=['mse', 'mape'],
        verbose=10
        #early_stopping_rounds=200
    )
    
    y_pred = lgbm_reg.predict(X_test)
    #Plotear grafica
    #plotGBTLoss(lgbm_reg, n_estimators)
    
    print("MSE: ", (mean_squared_error(y_test, y_pred)))
    print("RMSE: ", np.sqrt(mean_squared_error(y_test, y_pred)))
    print("MAE: ", mean_absolute_error(y_test, y_pred))
    print("MAPE: ", mean_absolute_percentage_error(y_test, y_pred))

    print("PREDICCION")
    print(len(y_pred))
    print(y_test)
    
    global y_pred_gbt
    y_pred_gbt = y_pred 
    
    print(lgbm_reg.best_score_)

    return mean_absolute_percentage_error(y_test, y_pred), 

GBT([0.27, 85, 5, 4.82, 0.32, 1.0, 7.16])
""" Plot ANN Loss Curve """
def plotANNLoss(history, neurons):
    plt.plot(history.history['loss'], color="#C0C0C0",
             label="Training loss")
    plt.plot(history.history['val_loss'], color="k",
             label="Validation loss")
  
    plt.legend(loc="best")
    plt.xlabel("Epochs")
    plt.ylabel("Loss (MSE)")
    plt.title("Loss Curve ANN (Neuronas Invisibles: {})".format(neurons))

"""# Artificial Neural Network"""
def runNeuralNetwork(weight_set_1, weight_set_2, neurons, activation1, activation2, X_train, X_valid, X_test, y_train, y_valid, y_test):
    
    if (activation1 == 0):
        activation1 = "relu"
    elif (activation1 == 1):
        activation1 = "tanh"
    elif (activation1 == 2):
        activation1 = "sigmoid"
        
    if (activation2 == 0):
        activation2 = "relu"
    elif (activation2 == 1):
        activation2 = "tanh"
    elif (activation2 == 2):
        activation2 = "sigmoid"
        
    pesos_1 = np.array(weight_set_1)
    pesos_1 = [np.reshape(pesos_1, (16,neurons))]

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(16, input_shape=X_train.iloc[0, :].shape))
    model.add(tf.keras.layers.Dense(neurons, activation=activation1, use_bias=False))
    #model.add(tf.keras.layers.Dropout(0.2))
    model.add(tf.keras.layers.Dense(1, activation=activation2))
      
  #Actualizar pesos capa entrada a capa invi 
    print("Pesos capa Input")
    print(model.layers[1].get_weights()[0])
    model.layers[1].set_weights(pesos_1)
    print("Nuevos pesos capa Input")
    print(model.layers[1].get_weights()[0])

    model.compile(loss=['mse'], metrics=['mape'])

    epochs = 100
  #Inicio de entrenamiento
    history = model.fit(      
        x=X_train.values,
        y=y_train,
        epochs=epochs,
        validation_data=(X_valid.values, y_valid),
        batch_size=500
    )
    print("History")
    #print(history.history)
  
    y_pred = model.predict(X_test.values)
  
    print("PREDICCION")
    print(y_pred)
  
    global y_pred_ann
    y_pred_ann = y_pred.flatten() 
  
    eval_model = model.evaluate(X_test.values, y_test)
    mape = eval_model[1]
  
    print("MSE: ", mean_squared_error(y_test, y_pred))
    print("RMSE: ", np.sqrt(mean_squared_error(y_test, y_pred)))
    print("MAE: ", mean_absolute_error(y_test, y_pred))
    print("MAPE: ", mape)
    #plotANNLoss(history, neurons)

    return mape
  
n_inputs = 16
#individuo_test = [2, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]
X_train, X_valid, X_test, y_train, y_valid, y_test = neuralNetwork()

def evalNeuralNetwork(individual): 
  #individuo_length = len(individual) - 1
  neurons = individual[0]
  activation1 = individual[1]
  activation2 = individual[2]
  set_1_length = neurons * n_inputs
  print(neurons)
  weight_set_1 = individual[3:set_1_length+3]
  weight_set_2 = individual[set_1_length+3::]
  costo = runNeuralNetwork(weight_set_1, weight_set_2, neurons, activation1, activation2, X_train, X_valid, X_test, y_train, y_valid, y_test)
  print("Weight set 1: {}".format(weight_set_1))
  print("Weight set 2: {}".format(weight_set_2))
  return int(costo),
evalNeuralNetwork([7, 0, 0, -0.21688617385625042, 1.0, 0.8659829248939577, 0.6717607535419003, -0.4203775018413698, -0.34515195379839514, 0.0, 0.32488569357677677, -0.7744621150220681, -0.5108464716551306, 0.942432117280021, -0.5327856064011098, -0.10353553586449515, 0.0, -0.3455554184015792, 0.4421885774521688, -0.7913248110387443, 0.0, -0.5516348366519204, 0.0, 0.8610956561854337, -0.7464985294590154, 0.09874255841790314, -0.242096775947225, -0.6192796749082672, 0.792006283413947, -0.05421115761174855, 0.05599640401525163, 0.09249024195306865, 0.8139922753363242, 1.0, 0.0, -0.3892825563748765, 0.8446150723274839, 0.8817143018742919, 0.6200287281572066, -0.04031501624672851, -0.8334326499287295, -0.1148856267954228, 0.013588325513820498, 0.586483536918091, 0.0, 0.0, 0.09017162861504091, -0.5572638254106215, -0.2613565075495994, -0.43149658868916263, -0.7087847112858847, 0.6586972096905976, 0.30840182273987793, -0.9020328859132212, 0.0, 0.2523949010402957, -0.7969997765645815, -0.20132511123199937, 0.9211358876158993, 0.194125927149428, 0.0, 0.6527122421177627, 0.0, -0.42839333986508366, -0.18093305589794118, 0.0, 0.7277716030074801, 0.0, 0.7968039368496687, -0.6722091724898565, 0.0, 0.0, -0.44257362977525716, 0.0, 0.0, -0.7214226341669214, -0.9983545438275099, 0.5137942517308309, -0.0918239173340405, 0.26800638839704227, 0.8009614048299738, -0.2726308563946067, -0.7877513878724787, 0.18899420188012495, 0.0, 0.15558042502479674, 0.0, 0.8464103880917493, 0.0, 0.8237778332943027, 0.16834406113614975, 0.8513087196557136, 0.6286137943258201, -0.805213814253239, 0.0, 0.7839677028101648, 0.18229839780920032, -0.6059868138322202, -0.4291770875436751, -0.26134149619174774, 0.0, -0.10151125699554031, 0.2028637466129759, 0.0, -0.4074312939305038, 0.042784983578392, 0.6278975296394165, 0.0976732096666606, 0.0, -0.39300994199806016, 0.49034575147900883, 0.3755295288950853, -0.26750170854395217, 0.2109318447805153, -0.77145941044815, -0.24700775140764297, 0.19424880135720102, -0.031306992651703114, 0.4997646896312158, -0.9857429994098306, 0.0, 0.09276247424891704])


print(y_pred_svm)
print(y_pred_gbt)
print(y_pred_ann)

dict = {'Pred_SVM': y_pred_svm, 'Pred_ANN': y_pred_ann, 'Pred_GBT': y_pred_gbt}

pred_df = pd.DataFrame(dict)
pred_df

X_test_cp = X_test.copy()

dict = {'Day': X_test_cp["day"], 'Month': X_test_cp["Month"], 'Year': X_test_cp["Year"]}
pred_dates = pd.DataFrame(dict)
pred_dates

pred_dates["Date"] = pd.to_datetime(pred_dates.Year*10000+pred_dates.Month*100+pred_dates.Day,format='%Y%m%d')
pred_dates["DP"] = X_test_cp["isHoliday"]
pred_dates
pred_dates_X = pred_dates.copy()
pred_dates_noIndex = pred_dates_X.reset_index(drop=True)
pred_dates_noIndex 

pred_df["Dates"] = pred_dates_noIndex["Date"]
pred_df["DP"] = pred_dates_noIndex["DP"]
pred_df

pred_df.insert(3,'Pred_Test', y_test.values)

pred_df = pred_df.sort_values(by='Dates')
pred_df = pred_df.reset_index(drop=True)
pred_df

pred_filter = pred_df.copy()

mask = (pred_df['Dates'] >= "2018-04-28") & (pred_df['Dates'] <= "2018-10-30")
pred_filter = pred_filter.loc[mask]
pred_filter

#ax = sns.lineplot(x=pred_filter["Dates"], y = pred_filter["Pred_SVM"])

#C0C0C0
plt.figure(figsize=(16, 6))
plt.plot(pred_filter["Dates"], pred_filter["Pred_Test"], label="Esperado (Test)", color="#C0C0C0", alpha=0.8)
#plt.plot(pred_filter["Dates"], pred_filter["Pred_SVM"], 'tab:green', label="Prediccion SVM")
plt.plot(pred_filter["Dates"], pred_filter["Pred_ANN"], 'tab:blue', label="Prediccion ANN")
#plt.plot(pred_filter["Dates"], pred_filter["Pred_GBT"], 'tab:orange', label="Prediccion GBT")

plt.xlabel("Fechas")
plt.ylabel("Ventas")
plt.title("Predicción (ANN) vs. Esperado")
plt.legend(loc="best")
plt.show()

fig, axs = plt.subplots(2, 2)
axs[0, 0].plot(pred_filter["Dates"], pred_filter["Pred_Test"], 'tab:gray')
axs[0, 0].set_title('Prediccion Esperada')
axs[0, 1].plot(pred_filter["Dates"], pred_filter["Pred_GBT"], 'tab:orange')
axs[0, 1].set_title('Pronostico (GBT)')
axs[1, 0].plot(pred_filter["Dates"], pred_filter["Pred_SVM"], 'tab:green')
axs[1, 0].set_title('Pronostico (SVM)')
axs[1, 1].plot(pred_filter["Dates"], pred_filter["Pred_ANN"], 'tab:red')
axs[1, 1].set_title('Pronostico (ANN)')

for ax in axs.flat:
    ax.set(xlabel='Fechas', ylabel='Ventas')

# Hide x labels and tick labels for top plots and y ticks for right plots.
for ax in axs.flat:
    ax.label_outer()

fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(10,4))
fig.suptitle('Pronostico de ventas de cada modelo')

line_labels = ['Test', 'SVM', 'GBT', 'ANN']

l1 = ax1.plot(pred_filter["Dates"], pred_filter["Pred_Test"], 'tab:gray')
l2 = ax2.plot(pred_filter["Dates"], pred_filter["Pred_SVM"], 'tab:green')
l3 = ax3.plot(pred_filter["Dates"], pred_filter["Pred_GBT"], 'tab:orange')
l4 = ax4.plot(pred_filter["Dates"], pred_filter["Pred_ANN"], 'tab:blue')

# Hide x labels and tick labels for top plots and y ticks for right plots.

ax1.label_outer()
ax2.label_outer()
ax3.label_outer()
fig.text(0.06, 0.5, 'Ventas', va='center', rotation='vertical')
plt.xlabel("Fechas")
<<<<<<< HEAD
fig.legend([l1, l2, l3, l4], labels=line_labels, loc='center right', title='Leyenda', fancybox=True, shadow=True)
=======
#fig.set_position([box.x0, box.y0 + box.height * 0.1,
                 #box.width, box.height * 0.9])
fig.legend([l1, l2, l3, l4], labels=line_labels, loc='center right', title='Leyenda', fancybox=True, shadow=True)
#ax.legend([l1, l2, l3, l4], labels=line_labels, loc='upper center', bbox_to_anchor=(0.5, -0.05),
          #fancybox=True, shadow=True, ncol=len(pred_filter["Dates"]))
>>>>>>> 28a3b047bb08db011cc2da5af2cb2b0f715757be
plt.show()

#Operadores Geneticos
#toolbox.register("evaluate", evalNeuralNetwork)
toolbox.register("evaluate", SVM)
#toolbox.register("evaluate", GBT)
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)

#population = toolbox.population(n=4)
print(population)
fitnesses = list(map(toolbox.evaluate, population))
print(fitnesses)

for ind, fit in zip(population, fitnesses):
      print(ind)
      print(fit)
      ind.fitness.values = fit

# CXPB = % probabilidad de Crossover
# MUTPB = % probabilidad de Mutation
CXPB, MUTPB = 0.5, 0.2

print("Population")
print(population)
fits = [ind.fitness.values[0] for ind in population]
print(fits)

generation = 0 
maxGenerations = 50
individuoAceptable = 5

while max(fits) > individuoAceptable and generation < maxGenerations: 
  generation = generation + 1 
  print("-- Generation %i --" % generation)
  print("-- Generation %i --" % generation)
  print("-- Generation %i --" % generation)
  print("-- Generation %i --" % generation)
  print("-- Generation %i --" % generation)
  # Select the next generation individuals
  offspring = toolbox.select(population, len(population))
  # Clone the selected individuals
  offspring = list(map(toolbox.clone, offspring))
  for child1, child2 in zip(offspring[::2], offspring[1::2]):
    if random.random() < CXPB:
      #Crossover
      toolbox.mate(child1, child2)
      del child1.fitness.values
      del child2.fitness.values

    for mutant in offspring:
      if random.random() < MUTPB:
        toolbox.mutate(mutant)
        del mutant.fitness.values
        # Evaluate the individuals with an invalid fitness
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_ind)
    print("invalid_ind: {}".format(invalid_ind))
    print("fitnesses: {}".format(fitnesses))
    for ind, fit in zip(invalid_ind, fitnesses):
      ind.fitness.values = fit

    print("Evaluated %i individuals" % len(invalid_ind))
    #Reemplazar poblacion antigua por nueva con los nuevos fitness 
    #de los individuos
    population[:] = offspring
    # Gather all the fitnesses in one list and print the stats
    fits = [ind.fitness.values[0] for ind in population]
        
    length = len(population)
    mean = sum(fits) / length
    sum2 = sum(x*x for x in fits)
    std = abs(sum2 / length - mean**2)**0.5
        
    print("  Min %s" % min(fits))
    print("  Max %s" % max(fits))
    print("  Avg %s" % mean)
    print("  Std %s" % std)

  print("-- End of (successful) evolution --")

print(population)

best_ind = tools.selBest(population, 1)[0]
print("Best individual is %s, %s" % (best_ind, best_ind.fitness.values))